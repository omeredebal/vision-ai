#!/bin/bash

# SmolVLM Server Start Script
# Uses llama.cpp server with Qwen2.5-VL model (better multilingual/Turkish support)

echo "ğŸš€ Qwen2.5-VL Vision Server baÅŸlatÄ±lÄ±yor..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Check if llama-server is installed
if ! command -v llama-server &> /dev/null; then
    echo "âŒ llama-server bulunamadÄ±!"
    echo ""
    echo "Kurulum iÃ§in:"
    echo "  brew install llama.cpp"
    echo ""
    exit 1
fi

echo "âœ“ llama-server bulundu"
echo ""

# Model configuration - Qwen2.5-VL with better multilingual support
MODEL_REPO="ggml-org/Qwen2.5-VL-3B-Instruct-GGUF"
MODEL_FILE="Qwen2.5-VL-3B-Instruct-Q8_0.gguf"
MMPROJ_URL="https://huggingface.co/ggml-org/Qwen2.5-VL-3B-Instruct-GGUF/resolve/main/mmproj-Qwen2.5-VL-3B-Instruct-f16.gguf"
PORT=8080
HOST="127.0.0.1"

echo "ğŸ“¦ Model: Qwen2.5-VL-3B-Instruct (Ã‡ok dilli, TÃ¼rkÃ§e destekli)"
echo "ğŸ”Œ Port: $PORT"
echo ""
echo "â³ Ä°lk Ã§alÄ±ÅŸtÄ±rmada model indirilecek (~2GB), lÃ¼tfen bekleyin..."
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Start llama-server with Qwen2.5-VL model
llama-server \
    --hf-repo "$MODEL_REPO" \
    --hf-file "$MODEL_FILE" \
    --mmproj-url "$MMPROJ_URL" \
    --host "$HOST" \
    --port "$PORT" \
    -ngl 99 \
    --ctx-size 4096
